#+title: [[https://pages.cs.wisc.edu/~remzi/OSTEP/][Operating Systems: Three Easy Pieces]]
#+subtitle: Part 3: Persistence - Reading Notes
#+author: Tahsin Ahmed
#+html_head: <link rel="stylesheet" href="../css/style.css" />

* I/O Devices
** System Architecture
- CPU attached to main memory via a *memory bus*.
- Faster devices connected via an *I/O bus* typically *PCI* or *PCIe* (i.e. GPU and SSD).
- Slower devices connected via *peripheral bus* like *SCSI*, *SATA* or *USB* (i.e. disks, mouse, keyboard).

** A Cannonical Device
- Devices have two main components:
  - An external hardware interface it presents to the rest of the system.
  - An internal structure used to implement the functions of the device.

** The Cannonical Protocol
- Device interfaces typically consist of three registers:
  - *Status* register to query the current state of the device.
  - *Command* register to issue commands to the device.
  - *Data* register to pass input and retrieve output.
- The CPU polls the device by repeatedly reading the status register.

** Lowering CPU Overhead With Interrupts
- Rather than polling, the device can interrupt the CPU using an *interrupt handler* when it is done.
  - Allows the CPU to run another task while waiting.

** More Efficient Data Movement With DMA
- Alternatively the device can also use *Direct Memory Access (DMA)* to directly transfer data to the main memory before interrupting the CPU.

** Methods Of Device Interaction
- Two ways to interact with devices:
  - Send explicit *I/O instructions* via specific device registers.
  - Access specified memory addresses via *memory mapped I/O* made available by the hardware.

** Fitting Into The OS: The Device Driver
- Device drivers used to abstract the intrinsics of communicating with devices.
- Up to 70% of the Linux kernel code is device driver code.

* Hard Disk Drives
** The Interface
- Hard disk can be viewed as an array of sectors numbered from \(0\) to \(n - 1\).
  - Writes to sectors are atomic.

** Basic Geometry
- Hard disk consists of *platters* bound together by a rotating *spindle*.
- Data contained within concentric circles around each platter known as a *track*.
- *Disk heads* per platter used to read and write data from tracks.

** A Simple Disk Drive
- Disk drive consists of three types of delays:
  - Rotational delay: The time needed to rotate the track to the specific sector.
  - Seek time: The time needed to move the disk head to the appropriate track.
  - Transfer time: The time needed to perform I/O operations (i.e. read / write).
- On reads disk may also read adjacent tracks which are placed in its internal cache.
- On writes disk may write to cache and defer writing to disk for later.

** I/O Time: Doing The Math
- Total I/O delay can be calculated using:
  \[T_{I/O} = T_{seek} + T_{rotation} + T_{transfer}\]
- The rate of I/O can be calculated using:
  \[R_{I/O} = \frac{Size_{transfer}}{T_{I/O}}\]

** Disk Scheduling
- *Shortest Seek Time First (SSTF)*: Services requests on tracks nearest to the current disk head.
  - May lead to starvation of sectors on farther tracks.
- *Elevator (SCAN)*: Service requests in order of most inner track to outer track and back again.
  - Strongly favors middle tracks because they are serviced twice for every one time an inner/outer track is serviced.
  - *C-SCAN*: Like SCAN, but restart at inner track each time. More fair to outer and inner tracks.
- *Shortest Positioning Time First (SPFT)*: Like SSTF but also considers rotational delay as well as seek time.
- Disk scheduling often done by the disk's firmware since the OS has no knowledge of the geometry of the disk.

* Redundant Disk Arrays (RAID)
- *Redundant Array of Inexpensive Disks (RAID)* is a way to combine disks to build bigger, faster and more reliable disks.

** Interface And RAID Internals
- RAID controller internally redirects I/O to disk, and externally presents a larger disk to the system.

** Fault Model
- In *fail-stop* fault model disks are either working or dead.

** How To Evaluate RAID
- RAID setup is evalulated along three axes:
  - *Capacity*: How much useful space is available to the user?
    - Let \(B\) be the max capacity of a single block.
  - *Reliability*: How many disk faults can the setup tolerate?
  - *Performance*: How well does the setup perform under a given workload?
    - We consider two workloads sequential and random.
    - Assume the max sequential bandwidth is \(S\) MB/s for a single disk.
    - Similarily max random bandwidth is \(R\) MB/s for single disk.

** RAID Level 0: Striping
- RAID-0 *stripes* logical blocks in a round robin fashion.

  #+caption: RAID-0: Striping
  | Disk 0 | Disk 1 | Disk 2 | Disk 3 |
  |--------+--------+--------+--------|
  | 0      | 1      | 2      | 3      |
  | 4      | 5      | 6      | 7      |
  | 8      | 9      | 10     | 11     |
  | 12     | 13     | 14     | 15     |

- Can also change the *chunk size* to reduce seek times between disks.

  #+caption: Striping with bigger chunks
  | Disk 0 | Disk 1 | Disk 2 | Disk 3 |
  |--------+--------+--------+--------|
  | 0      | 2      | 4      | 6      |
  | 1      | 3      | 5      | 7      |
  | 8      | 10     | 12     | 14     |
  | 9      | 11     | 13     | 15     |

- Capacity: \(N * B\).
- Reliability: \(0\).
- Performance:
  - Sequential: \(N * S\) MB/s for both reads and writes.
  - Random: \(N * R\) MB/s for both reads and writes.

** RAID Level 1: Mirroring
- RAID-1 *mirrors* disk onto another disk.

  #+caption: RAID-1: Mirroring
  | Disk 0 | Disk 1 | Disk 2 | Disk 3 |
  |--------+--------+--------+--------|
  | 0      | 0      | 1      | 1      |
  | 2      | 2      | 3      | 3      |
  | 4      | 4      | 5      | 5      |
  | 6      | 6      | 7      | 7      |

  - Can arrange disksin different ways:
    - *RAID-10*, *stripe of mirrors*
    - *RAID-01*, *mirror of stripes*

- Capacity: \(\frac{N}{2} * B\).
- Redundancy: \(\frac{N}{2}\).
- Performance:
  - Sequential:
    - Writes: \(\frac{N}{2} * S\) MB/s because we need to write to both disks for a single block.
    - Reads: \(\frac{N}{2} * S\) MB/s also.
      - Assume that we alternate when sending I/O. Send read of 0 to disk 0, read of 1 to disk 2, read of 2 to disk 0, etc.
      - Each disk skips over every other blocks and thus only achieves half its total bandwidth.
  - Random:
    - Writes: \(\frac{N}{2} * R\) MB/s.
    - Reads: \(N * R\) MB/s.

** RAID Level 4: Saving Space With Parity
- RAID-4 achieves redundancy by dedicating one disk to storing the *parity* of each stripe. The partiy is the bitwise XOR of all blocks in a stripe.

  #+caption: RAID-4: Parity
  | Disk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4 |
  |--------+--------+--------+--------+--------|
  | 0      | 1      | 2      | 3      | P0     |
  | 4      | 5      | 6      | 7      | P1     |
  | 8      | 9      | 10     | 11     | P2     |
  | 12     | 13     | 14     | 15     | P3     |

- Capacity: \((N-1) * B\).
- Redundancy: \(1\).
- Performance:
  - Sequential:
    - Writes: \((N-1) * S\) MB/s, entire stripe is written at once so parity is computed and written at write time.
    - Reads: \((N-1) * S\) MB/s, one disk contains redundant parity information.
  - Random:
    - Writes: \(\frac{R}{2}\) MB/s, parity disk is bottleneck and must be updated with each block update, even if the blocks updated are on separate disks. Thus these updates cannot happen in parallel and therefore cannot be amortized.
    - Reads: \((N-1) * R\) MB/s.

** RAID Level 5: Rotating Parity
- RAID-5 is idential to RAID-4 except that it rotates the parity block between drives.

  #+caption: RAID-5: Rotating Parity
  | Disk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4 |
  |--------+--------+--------+--------+--------|
  | 0      | 1      | 2      | 3      | P0     |
  | 5      | 6      | 7      | P1     | 4      |
  | 10     | 11     | P2     | 8      | 9      |
  | 15     | P3     | 12     | 13     | 14     |
  | P4     | 16     | 17     | 18     | 19     |

- Capacity: \((N-1) * B\).
- Redundancy: \(1\).
- Performance:
  - Sequential:
    - Writes: \((N-1) * S\) MB/s.
    - Reads: \((N-1) * S\) MB/s.
  - Random:
    - Writes: \(\frac{N}{4} * R\) MB/s, still generates \(4\) I/O requests.
    - Reads: \(N * R\) MB/s, can utilize all disks unlike RAID-4.

* Interlude: Files and Directories
** Files And Directories
- Two abstractions provided by the file system:
  - A *file* is a linear array of bytes.
  - A *directory* contains files.
  - Both are backed by inodes.

** The File System Interface
- Unix provides a common interface to access the file system.

** Creating Files
- Files can be created by using ~open()~ with the ~O_CREAT~ flag.

  #+begin_src C
int fd = open("foo", O_CRAT|O_WRONLY|O_TRUNC, S_IRUSR|S_IWUSR);
  #+end_src

  - Returns a file descriptor which can later be used to access the file.
    - File descriptors are private to each process.

** Reading And Writing Files
- *strace* program can be used to trace syscalls in Linux.
- Use ~read()~ and ~write()~ with the file descriptor as an argument to read and write to file.

** Reading And Writing, But Not Sequentially
- Can seek to random location within the file using ~lseek()~.

  #+begin_src C
off_t lseek(int fildes, off_t offset, int whence);
  #+end_src

  - The offset is w.r.t. to a set location within the file:
    - ~SEEK_SET~ for beginning of the file.
    - ~SEEK_CUR~ for current offset within the file.
    - ~SEEK_END~ for end of the file.

- OS keeps track of per file offset for each process.

  #+begin_src C
struct file {
    int ref;
    char readable;
    char writeable;
    struct inode *ip;
    uint off;
}
  #+end_src

- Aside: Also possible to map file offsets to memory addresses using ~mmap()~.

** Shared File Table Entries: ~fork()~ And ~dup()~
- When a parent process forks it copies the file table to the child process.
  - Increases the *reference count* of the underlying file object.
- The ~dup()~ sycalls (and its friends ~dup2()~ and ~dup3()~) is used to assign a new fd to an already existing open file.
  - Used by the shell to redirect input and output.

** Writing Immediately With ~fsync()~
- The file system will often buffer writes in memory to achieve better performance.
- A process can force writes to a file using ~fsync()~.

** Renaming Files
- Files can be renamed using the ~rename()~ syscall (or the ~mv~ command in the shell).

** Getting Information About Files
- Metadata about a file can be obtained using the ~sync()~ or ~fsync()~ syscalls.
  - Metadata stored within a ~struct stat~.

** Removing Files
- File can be removed using the ~unlink()~ syscall. But why not ~remove~ or ~delete~?

** Making Directories
- To create a directory use the ~mkdir()~ syscall.

** Reading Directories
- Directories are special files maintained by the file system. Can be accessed using the ~opendir()~, ~readdir()~ and ~closedir()~ syscalls.

** Deleting Directories
- To delete a directory use the ~rmdir()~ syscall.

** Hard Links
- A hard link within the file system can be created using the ~link()~ syscall (or ~ln~ in the shell).
  - Adds another entry within a directory pointing to an already existing inode.
  - Inode keeps a reference count of the number of directories containing it.
  - The inode is freeded only once its reference count reaches zero, hence why the remove syscall is called unlink.

** Symbolic Links
- Hard links are somewhat limited:
  - Cannot create hard link to directory due to infinite recursion.
  - Cannot create hard link across mounted file systems.
- A *symbolic link* or *soft link* maintains its own inode, but its contents point to another path.
  - Can be created using ~ln -s~ flag in the shell.
  - May potentially lead to a dangling reference.

** Permission Bits And Access Control Lists
- *Permission bits* used to control who has access to which files.
- Three permission groups: owner, group and other.
- Three permission modes: read, write and execute.
- Distributed file systems may use *access control lists* for more fine-grained access.

** Making And Mounting A File System
- A new file system can be formatted onto a device using ~mkfs~.
- The ~mount()~ / ~umount()~ syscalls can be used to mount a device formatted with a file system on a directory.

* File System Implementation
** The Way To Think
- All file systems consist of two parts:
  - What are the on-disk data structures used to organize metadata and data?
  - How is the file system data accessed?

** Overall Organization
- File system divides the disk into roughly four parts:
  - *Superblock*: Stores file system metadata (i.e. what type of file system).
  - *Bitmaps*: Indicates which blocks are valid or invalid.
  - *Inode table*: Stores inode metadata (i.e. size, type etc).
  - *Data Region*: Stores file contents.

** File Organization: The Inode
- Each file is backed by a low-level name called an i-number.
- Each inode in the inode table stores several *direct pointers* to data.
  - The last pointer is an *indirect pointer* to a data block containing more direct pointers.
  - Can also have *multi-level indirect pointers*.

** Directory Organization
- Directories are special types of "files" maintained by the file system containing mapping from names to inodes.
  - Every directory contains at least two entries =.= and =..= referring to the current and parent directory respectively.

** Free Space Management
- Some file systems pre-allocate data blocks to files to ensure that they are contigious.

** Access Paths: Reading and Writing
- Reading a file from disk:
  1. Resolve the pathname to find the corresponding inode. Requires traversing the file system. Traversal starts at the root node =/=.
  2. Lookup the inode in the inode table using its i-number.
  3. Find the data block for the offset by following direct (or indirect) pointers.
- Writing a file to disk:
  - Same as reading from a disk except that blocks maybe allocated.

** Caching and Buffering
- File system may cache popular blocks in main memory.
  - *LRU cache* used to evict least recently block from cache.
- *Write buffering* used to batch multiple writes to a block as one I/O request.

* Loaclity and The Fasl File System
** The Problem: Poor Performance
- Disk is not a random access device!
- Overtime disks can become fragmented reducing performance.

** FFS: Disk Awareness Is The Solution
- *Fast File System (FFS)* designed to be improve performance by being "disk aware".

** Organizing Structure: The Cylinder Group
- *Cylinder group* refers to tracks on different platters or surfaces that are the same distance from the center.
- Approximate cylinder groups by dividing the disk into *block groups*.
  - Each block contains a copy of the super block for reliability reasons.
  - Its own bitmap, inode table and data blocks.

** Policies: How To Allocate Files and Directories
- For directories choose the block group with the least amount of allocated directories and most amount of free inodes.
- For files try to allocate data blocks in the same BG. Also try to allocate files in the same directory in the same BG.

** Measuring File Locality
- Most file access is either to the same file or to another file in the same directory.

** The Large-File Exception
- To avoid a large file taking up an entire BG, when a file becomes big enough FFS places the next "large" chunk in another BG.

** A Few Other Things About FFS
- FFS also supported small files using *sub-blocks* which can be merged into full blocks.

* Crash Consistency: FSCK and Journaling
- File system updates often involve writes to multiple blocks. How to make sure all blocks are updates in a consistent manner in the event of a power loss?

** A Detailed Example
- An append to an inode involves:
  - An allocation of a new block and writing data to it.
  - An update to the inode to add the new pointer.
  - An update to the bitmap to mark the block as allocated.

** Solution #1: The File System Checker
- Unix provides *fsck* tool for finding and repairing inconsistencies in the file system.
  - Check superblock for corruption.
  - Scan inodes, indirect blocks and double indirect block to build understanding of file system and of allocated blocks.
  - Check each inode for corruption (i.e. is the type field valid?).
  - Check reference count of inodes.
  - Check for duplicate pointers to data between inodes.
  - Check for bad block pointers (i.e. if it is out of range).
  - Check directories (i.e. do they have =.= and =..= directories).
- Fsck must read through entire file system and validate obviously valid blocks. Slow.

** Solution #2: Jounaling (or Write-Ahead Logging)
- Reserve space on the storage media for a *journal*.
  1. *Journal write*: Write transaction begin followed by all writes (metadata + data).
  2. *Journal commit*: Write transaction end. Must be issued after entire transaction is written to journal to avoid early commit.
  3. *Checkpoint*: Update on-disk metadata and data blocks.
  4. *Free*: At some point later free the journal for future use.
- In the case of a crash, if a transaction is committed the file system replays writes on boot.
- Journaling forces us to write data twice: once to the journal and once to disk.
  - *Metadata journaling*: Update the data first and then journal the metadata as a transaction.
  - Avoids rewritting the data (the bulk of the update) and ensures the metadata is left in a consistent state.
- Special *revoke* record to discard transactions to handle edge cases of block reuse.

** Solution #3: Other Approaches
- Never delete data. Keep older versions and use *copy-on-write*. Used by Sun's ZFS.
- *Backpointer-based consistency*: Add backpointer to inode in each data block to check if data block is still valid.

* Log-structured File Systems
- *Log-structured file systems* take advantage of the sequential writes of a disk.

** Writing To Disk Sequentially
- Inode is written directly after the data and points directly to head of data.

** Writing Sequentially and Effectively
- LFS uses *write buffering* to write blocks at once rather than immediately.
  - Prevents missing blocks due to rotation of the disk.

** How Much To Buffer?
- The amount to buffer must be choosen carefully in order to achieve a target effective transfer rate.
  - The time to write is:
    \[T_{write} = T_{position} + \frac{D}{R_{peak}}\]
  - The effective disk transfer rate is:
    \[R_{effective} = \frac{D}{T_{write}} = \frac{D}{T_{position} + \frac{D}{R_{peak}}}\]
  - Let \(F\) be the fraction of the peak rate we wish to achieve. Rearranging we can solve for \(D\):
    \[D = \frac{F}{1 - F} \times R_{peak} \times T_{position}\]

** Problem: Finding Inodes
- In typical file system finding an inode is easy because they are in a central location, but in LFS they are scattered.

** Solution Through Indirection: The Inode Map
- Use an inode map to map i-numbers to the latest version of the inode on disk.
- Where to store imap? Cannot store in central location due to expensive seeks.
  - Store fragments or updates to imap sequentially after updated inodes.

** Completing The Solution: The Checkpoint Region
- Still need a known location to store location of imap.
- LFS *checkpoint region (CR)* stores addresses to the latest pieces of the imap. Updated periodically (i.e. every 30sec).

** Reading A File From Disk: A Recap
- On boot, LFS reads the entire imap to memory.
- On read, LFS consults the imap to find the location of the latest version of the inode.
- It then reads the data associated with the inode using direct and indirect pointers.

** What About Directories?
- When creating a file, the directory containing the file must also be updated.

** A New Problem: Garbage Collection
- LFS produces many old versions of files dubbed *garbage*.
  - Can keep old versions of files as a *versioning file system*.
  - Periodically find and collect garbage data.
    - Cleaning garbage data may result in holes.
    - Better to copy out unchanged portion of file to end of log and clean entire inode.

** Determining Block Liveness
- Each data block stores the inode and offset corresponding to it in the *segment summary block*.
  - Can be used to check if a data block is dead or alive by checking the referred inode.
- LFS can also keep a version number for each inode in the imap.

** A Policy Question: Which Blocks To Clean, And When?
- Segregate segments as /hot/ or /cold/ and only clean cold segments.

** Crash Recovery And The Log
- LFS uses *roll forward* to rebuild uncommitted segments.
  - LFS finds the end of the log by looking it up in the CR and reading ahead to find valid blocks.

* Flash-based SSDs
- *Solid-state storage* is built using transistors and has no moving parts unlike hard disks.

** Storing a Single Bit
- Flash chips trap charge within transistors.
  - *Single-level cell* stores one bit per cell.
  - *Multi-level cell* stores two bits per cell.
  - *Triple-level cell* stores three bits per cell.

** From Bits to Banks/Planes
- Flash chips organized into sets of *banks*. Each bank contains *blocks* which in turn contain *pages*.

** Basic Flash Operations
- *Read (a page)*: Reading any location on a flash chip is fast and random access.
- *Erase (a block)*: Before writing to a page, the entire block containing the page must be erased.
- *Program (a page)*: After erasure each page within a block can be individually programmed with data.

** Flash Performance And Reliability
- Flash contains no moving parts and therefore no possiblity of head crash.
- However, each flash page can only be erased and written a set number of times before *wear out*.
- Flash cells may also suffer from *read* or *program disturbances* from nearby cells.

** From Raw Flash to Flash-Based SSDs
- Flash chips utilize a *flash translation layer* or *FTL* to translate from /logical/ blocks to /physical/ blocks. This is used to remap pages on erasure.
- Two main goals with flash:
  - Increase performance by reducing the *write amplification* associated with erasing blocks and remapping pages.
  - Increase reliability by equally using each flash cell evenly also known as *wear leveling*.

** FTL Organization: A Bad Approach
- *Direct mapping* or mapping each logical page to exactly one physical page bad idea for flash.
  - Must erase entire block and copy it along with the changes to the page.
  - Hot pages are worn out faster than colder ones. Very user dependent.

** A Log-Structured FTL
- Writes to pages are redirected to the next available page. FTL maps to the latest version of the page.
- *Garbage collector* frees blocks once most of the pages within the block are dead.

** Garbage Collection
- How to find which pages to garbage collect?
  - Store LBA information within each page and check FTL if this is the current PBA for the LBA.
- Some SSDs overprovision space dedicated to garbage collection.

** Mapping Table Size
- *Page-level* FTL takes up too much space.
- *Block-level* FTL stores mappings from logical blocks to physical blocks. Upper bits used to index into block and lower bits used as an offset, similar to page tables in memory.
  - Suffers from small-write problem. Write to single page requires copying entire block.
- Use a *hybrid mapping* approach:
  - Reserve some blocks as *log blocks* and redirect all writes to pages to them. Keep a per-page mapping for each block.
  - Once enough sequential pages are written transfer them to data blocks.
    - *Switch merge*: All pages of the block are written in the same order they were written to the block.
    - *Partial merge*: Some of the pages of the block are written to.
    - *Full merge*: Different pages from different blocks are written to.

* Data Integrity and Protection
** Data Integrity and Protection
- Two types of failure modes:
  - *Fail-stop*
  - *Fail-partial*
    - *Latent sector errors (LSEs)*: one of more blocks cannot be read by the disk.
    - *Block corruption*: disk block is corrupted in a way not detectable disk itself.

** Handling Latent Sector Errors
- Storage controller can detect LSEs if it fails to read block and use whatever redundancy mechanism it has to recover.

** Detecting Corruption: The Checksum
- *Checksum* is the result of a function applied to a block. Used to verify the integrity of a block.
- Types of checksums:
  - XOR-based checksums: XOR chuncks of block data. Can only resist a unit change.
  - *Fletcher's checksum*: Let block \(D\) consist of bytes \(d_1 \dots d_n\). Then let \(s1 = s1 + d_i mod 255\) and \(s2 = s2 + s1 mod 255\) for all \(d_i\).
  - *Cyclic Redundancy Check (CRC)*: Treat \(D\) as large binary number of divide by an agreeded upon value \(k\).
- Two ways to layout checksums on disk:
  - Prepend checksums to the beginning of each corresponding block.
  - Separate block for checksum for group of sequential blocks.

** Using Checksums
- To use checksums compare the computed checksum with the stored checksum to verify integrity.

** A New Problem: Misdirected Writes
- A *midirected write* is a correct write to disk, but at the wrong location.
- Solution: Add *physical identifier* to checksum to verify write to correct location.

** One Last Problem: Lost Writes
- Sometimes a storage system may claim a block is written without it being persisted, known as a *lost write*.
- Solution: After successful write perform a *read-after-write* to verify data persists.

** Scrubbing
- *Disk scrubbing* refers to when a storage system periodically checks every block to verify its integrity.

** Overheads of Checksumming
- Two kinds of overheads associated with checksum:
  - Space overhead: Checksums take space on both disk and in memory.
  - Time overhead: Checksums can be expensive for a CPU to compute.
